{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\123\\Desktop\\human vasculature\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_24640\\525301475.py:10: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "import torch.nn as nn  \n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import cv2\n",
        "import os,sys\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parallel import DataParallel\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda import amp\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import gc\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import segmentation_models_pytorch as smp\n",
        "import time\n",
        "\n",
        "from segmentation.scr.utils.metrics import dice_coef\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from segmentation.config import Configs as CFG\n",
        "from segmentation.models.unet import unet\n",
        "from segmentation.scr.utils import losses, transforms, rle_coding\n",
        "from segmentation.scr.utils.utils import set_seed, save_model\n",
        "from segmentation.scr.tilling_dataset import Tilling_Dataset\n",
        "from colorama import Fore, Back, Style\n",
        "c_  = Fore.GREEN\n",
        "sr_ = Style.RESET_ALL\n",
        "\n",
        "#from segmentation.scr\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CFG:\n",
        "\n",
        "    # configs for tilling dataset kidney 1\n",
        "    path_img_kidney1 = \"data\\\\train\\\\kidney_1_dense\\\\images\"\n",
        "    path_lb_kidney1 = \"data\\\\train\\\\kidney_1_dense\\\\labels\"\n",
        "    path_df_kidney_1_till = \"data\\\\kidney_1_tilling.csv\"\n",
        "    tile_size = (256, 256)\n",
        "    overlap_pct = 0\n",
        "    cache_dir = \"data\"\n",
        "    mean_till1 = (0.2425, 0.2425, 0.2425)\n",
        "    std_till1 = (0.1766, 0.1766, 0.1766)\n",
        "\n",
        "    # configs for tilling dataset kidney 3\n",
        "    path_img_kidney3 = \"data\\\\train\\\\kidney_3_sparse\\\\images\"\n",
        "    path_lb_kidney3 = \"data\\\\train\\\\kidney_3_dense\\\\labels\"\n",
        "    path_df_kidney_3_till = \"data\\\\kidney_3_tilling.csv\"\n",
        "    tile_size = (256, 256)\n",
        "    overlap_pct = 0\n",
        "    cache_dir = \"data\"\n",
        "\n",
        "    # configs for transforms\n",
        "    p_rot = 0.3\n",
        "    p_aug = 0.3\n",
        "    # cofigs for train / eval model\n",
        "    random_seed = 0\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    train_batch_size = 2\n",
        "    n_accumulate = max(1, 8 // train_batch_size)\n",
        "    valid_batch_size = train_batch_size * 2\n",
        "    clip_norm=5\n",
        "    epochs = 28\n",
        "    lr = 3e-4\n",
        "    dice_th = 0.5\n",
        "    \n",
        "    path_to_save_state_model = \"weight\"\n",
        "    path_weight_model = \"weight\"\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform  = transforms.get_transform(transform_type='strong')\n",
        "val_transform = transforms.get_transform(transform_type='val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contains 20744 empty and 33952 non-empty tiles.\n",
            "Sample 0 empty and 64 non-empty tiles.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Tilling_Dataset(\n",
        "    name_data='kidney_1_tilling',\n",
        "    path_to_df=CFG.path_df_kidney_1_till,\n",
        "    use_random_sub=True,\n",
        "    empty_tile_pct=0,\n",
        "    sample_limit=64,\n",
        "    random_seed=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contains 14595 empty and 6447 non-empty tiles.\n",
            "Sample 6 empty and 58 non-empty tiles.\n"
          ]
        }
      ],
      "source": [
        "val_dataset = Tilling_Dataset(\n",
        "    name_data='kidney_3_tilling',\n",
        "    path_to_df=CFG.path_df_kidney_3_till,\n",
        "    use_random_sub=True,\n",
        "    empty_tile_pct=10, \n",
        "    sample_limit=65,\n",
        "    random_seed=CFG.random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contains 14595 empty and 6447 non-empty tiles.\n",
            "Sample 3 empty and 28 non-empty tiles.\n"
          ]
        }
      ],
      "source": [
        "val_dataset = Tilling_Dataset(\n",
        "    name_data='kidney_3_tilling',\n",
        "    path_to_df=CFG.path_df_kidney_3_till,\n",
        "    use_random_sub=True,\n",
        "    empty_tile_pct=10,\n",
        "    sample_limit=32,\n",
        "    random_seed=CFG.random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(CFG.random_seed)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.train_batch_size, num_workers=2, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(train_dataset, batch_size=CFG.valid_batch_size, num_workers=2, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = unet.UNet(n_channels=3, n_classes=1, bilinear=False).to(CFG.device)\n",
        "num_epoch = 4\n",
        "loss_fn = losses.BCE_DICE(mode=\"BCE\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "sheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max',factor=0.9)\n",
        "device = CFG.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_loop(model, optimizer, loss_func, train_loader, device=CFG.device, grad_clip = None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    dataset_size = 0\n",
        "    optimizer.zero_grad()\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Train ')\n",
        "    for step, batch in pbar:\n",
        "        images, masks,_, _ = batch\n",
        "        images = images.to(device, dtype=torch.float)\n",
        "        masks  = masks.to(device, dtype=torch.float)\n",
        "        \n",
        "        batch_size = images.shape[0]\n",
        "        dataset_size += batch_size\n",
        "    \n",
        "        y_pred = model(images)\n",
        "            \n",
        "        loss   = loss_func(y_pred, masks)\n",
        "        loss   = loss / CFG.n_accumulate\n",
        "        loss.backward() #loss.backward()  # backward-pass\n",
        "\n",
        "        running_loss += loss.item() * batch_size\n",
        "        dataset_size += batch_size\n",
        "        del images\n",
        "        del masks\n",
        "        del y_pred\n",
        "        \n",
        "        if (step + 1) % CFG.n_accumulate == 0 or (step + 1 == len(train_loader)):\n",
        "            if grad_clip:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CFG.clip_norm)\n",
        "            optimizer.step()  # update weights\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix( epoch=f'{step + 1}',\n",
        "                          train_loss=f'{running_loss / dataset_size:0.4f}',\n",
        "                          #train_dice = f'{train_dice:0.4f}',\n",
        "                          #train_jaccard = f'{train_jaccard:0.4f}',\n",
        "                          lr=f'{current_lr:0.5f}',\n",
        "                          gpu_mem=f'{mem:0.2f} GB')\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return epoch_loss\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:10<00:00,  3.06it/s, epoch=32, gpu_mem=1.65 GB, lr=0.00010, train_loss=0.0612]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0612151570385322"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_one_loop(model=model,optimizer=optimizer, train_loader=train_loader,\n",
        "               loss_func=loss_fn, device=CFG.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def valid_one_epoch(model, dataloader, loss_func,device=CFG.device):\n",
        " #(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    val_scores = []\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
        "    for _, batch in pbar:\n",
        "        images, masks,_, _ = batch \n",
        "        images  = images.to(device, dtype=torch.float)\n",
        "        masks   = masks.to(device, dtype=torch.float)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            y_pred  = model(images)\n",
        "            loss    = loss_func(y_pred, masks)\n",
        "\n",
        "        running_loss += loss.item() * batch_size\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        val_dice = dice_coef(y_pred=y_pred, y_true=masks).cpu().detach().numpy()\n",
        "        val_scores.append([val_dice])\n",
        "        del(images)\n",
        "        del(masks)\n",
        "        del(y_pred)\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        pbar.set_postfix(valid_loss=f'{running_loss / dataset_size:0.4f}',\n",
        "                        gpu_memory=f'{mem:0.2f} GB')\n",
        "    val_score  = np.mean(val_scores, axis=0)\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return epoch_loss, val_score[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Valid : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:05<00:00,  3.14it/s, gpu_memory=1.08 GB, valid_loss=0.4299]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4299070443958044, 0.026378512)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_one_epoch(model=model, loss_func=loss_fn, dataloader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:08<00:00,  3.82it/s, epoch=32, gpu_mem=1.91 GB, lr=0.00010, train_loss=0.0321]\n",
            "Valid : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:05<00:00,  3.18it/s, gpu_memory=1.08 GB, valid_loss=0.2473]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #1 train loss: 0.032\n",
            "Epoch #1 val loss: 0.247\n",
            "Epoch #1 dice_metric: 0.018457788974046707\n",
            "\u001b[32mValid loss Improved (-inf ---> 0.018457788974046707)\n",
            "Took 0.237 minutes for epoch 1\u001b[0m\n",
            "Epoch 2/3\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:08<00:00,  3.86it/s, epoch=32, gpu_mem=1.91 GB, lr=0.00010, train_loss=0.0312]\n",
            "Valid : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:04<00:00,  3.21it/s, gpu_memory=1.08 GB, valid_loss=0.2531]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #2 train loss: 0.031\n",
            "Epoch #2 val loss: 0.253\n",
            "Epoch #2 dice_metric: 0.024851622059941292\n",
            "\u001b[32mValid loss Improved (0.018457788974046707 ---> 0.024851622059941292)\n",
            "Took 0.234 minutes for epoch 2\u001b[0m\n",
            "Epoch 3/3\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:08<00:00,  3.88it/s, epoch=32, gpu_mem=1.91 GB, lr=0.00010, train_loss=0.0306]\n",
            "Valid : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:04<00:00,  3.22it/s, gpu_memory=1.08 GB, valid_loss=0.2439]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #3 train loss: 0.031\n",
            "Epoch #3 val loss: 0.244\n",
            "Epoch #3 dice_metric: 0.03110937774181366\n",
            "\u001b[32mValid loss Improved (0.024851622059941292 ---> 0.03110937774181366)\n",
            "Took 0.233 minutes for epoch 3\u001b[0m\n",
            "Training complete in 0h 0m 42s\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, \n",
        "                optimizer, \n",
        "                loss_func, \n",
        "                train_loader, \n",
        "                val_loader,\n",
        "                num_epochs=1,\n",
        "                grad_clip = None , \n",
        "                scheduler=None, \n",
        "                device =CFG.device,\n",
        "                path_to_save=CFG.path_to_save_state_model,\n",
        "                \n",
        "):\n",
        "    \n",
        "    best_metric      = -np.inf\n",
        "    loss_mas, metrics_mas,train_losses, val_losses = [], [], [],[]\n",
        "    best_epoch = 0\n",
        "    total_time = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "        train_loss = train_one_loop(model=model, \n",
        "                                    optimizer=optimizer, \n",
        "                                    loss_func=loss_func,\n",
        "                                    train_loader=train_loader,\n",
        "                                    grad_clip=grad_clip,\n",
        "                                    device=device)\n",
        "        val_loss, dice_metric = valid_one_epoch(model=model,\n",
        "                                                loss_func=loss_fn,\n",
        "                                                dataloader=val_loader,\n",
        "                                                device=device\n",
        "                                                )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        metrics_mas.append(dice_metric)\n",
        "        if scheduler:\n",
        "            if scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
        "                scheduler.step(dice_metric)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "        if loss_func.__class__.__name__:\n",
        "            loss_func.update_n()\n",
        "            \n",
        "        # deep copy the model\n",
        "        print(f\"Epoch #{epoch+1} train loss: {train_loss:.3f}\")\n",
        "        print(f\"Epoch #{epoch+1} val loss: {val_loss:.3f}\")\n",
        "        print(f\"Epoch #{epoch+1} dice_metric: {dice_metric}\")\n",
        "        loss_mas.append(losses)\n",
        "        \n",
        "        if dice_metric > best_metric:\n",
        "            print(f\"{c_}Valid metrics Improved ({best_metric} ---> {dice_metric})\")\n",
        "            best_metric    = dice_metric\n",
        "            save_model(\n",
        "                model=model,\n",
        "                optimizer=optimizer,\n",
        "                model_name=model.__class__.__name__ +\"_best_model_at_\" + str(epoch + 1),\n",
        "                path=path_to_save,\n",
        "                lr_scheduler=scheduler,\n",
        "            )\n",
        "        \n",
        "        with open(\"train_results.txt\", \"w\") as file_handler:\n",
        "            file_handler.write(\"train_loss\\n\")\n",
        "            for item in train_losses:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "\n",
        "            file_handler.write(\"\\nval_loss\\n\")\n",
        "            for item in val_losses:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "\n",
        "            file_handler.write(\"\\ndice_metric\\n\")\n",
        "            for item in metrics_mas:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "        \n",
        "        \n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        print(f\"{sr_}Took {((end - start) / 60):.3f} minutes for epoch {epoch + 1}\")\n",
        "        \n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        total_time // 3600, (total_time % 3600) // 60, (total_time % 3600) % 60))\n",
        "\n",
        "        \n",
        "        \n",
        "train_model(model=model, \n",
        "                optimizer=optimizer, \n",
        "                loss_func=loss_fn, \n",
        "                train_loader=train_loader, \n",
        "                val_loader=val_loader, \n",
        "                num_epochs=3, \n",
        "                scheduler=sheduler, \n",
        "                device =CFG.device,\n",
        "                path_to_save=CFG.path_to_save_state_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_fn.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
