{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from segmentation.scr.utils.utils import set_seed\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation.scr.utils import losses, transforms\n",
        "from segmentation.models.unet import unet\n",
        "\n",
        "from segmentation.scr.utils import losses, transforms\n",
        "from segmentation.scr.utils.utils import set_seed\n",
        "from segmentation.scr.utils.metrics import dice_coef\n",
        "from segmentation.scr.utils.utils import save_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    # ============== pred target =============\n",
        "    target_size = 1\n",
        "\n",
        "    # ============== model CFG =============\n",
        "    model_name = 'Unet'\n",
        "    backbone = 'se_resnext50_32x4d'\n",
        "\n",
        "    in_chans = 3 # 65\n",
        "    # ============== training CFG =============\n",
        "    image_size = 512\n",
        "    input_size=512\n",
        "\n",
        "    train_batch_size = 4\n",
        "    n_accumulate = max(1, 16 // train_batch_size)\n",
        "    valid_batch_size = train_batch_size * 2\n",
        "\n",
        "    epochs = 20\n",
        "    lr = 3e-4\n",
        "    chopping_percentile=1e-3\n",
        "    # ============== fold =============\n",
        "    valid_id = 1\n",
        "\n",
        "\n",
        "    # ============== augmentation =============\n",
        "    train_aug_list = [\n",
        "        A.Rotate(limit=45, p=0.5),\n",
        "        A.RandomScale(scale_limit=(0.8,1.25),interpolation=cv2.INTER_CUBIC,p=0.5),\n",
        "        A.RandomCrop(input_size, input_size,p=1),\n",
        "        A.RandomGamma(p=0.75),\n",
        "        A.RandomBrightnessContrast(p=0.5,),\n",
        "        A.GaussianBlur(p=0.5),\n",
        "        A.MotionBlur(p=0.5),\n",
        "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "    ]\n",
        "    train_aug = A.Compose(train_aug_list)\n",
        "    valid_aug_list = [\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "    ]\n",
        "    valid_aug = A.Compose(valid_aug_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Data_loader(Dataset):\n",
        "    def __init__(self,paths,is_label):\n",
        "        self.paths=paths\n",
        "        self.paths.sort()\n",
        "        self.is_label=is_label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        img=cv2.imread(self.paths[index],cv2.IMREAD_GRAYSCALE)\n",
        "        img=torch.from_numpy(img)\n",
        "        if self.is_label:\n",
        "            img=(img!=0).to(torch.uint8)*255\n",
        "        else:\n",
        "            img=img.to(torch.uint8)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHOPPING_PER = 1e-3\n",
        "def min_max_normalization(x:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"input.shape=(batch,f1,...)\"\"\"\n",
        "    shape=x.shape\n",
        "    if x.ndim>2:\n",
        "        x=x.reshape(x.shape[0],-1)\n",
        "    \n",
        "    min_=x.min(dim=-1,keepdim=True)[0]\n",
        "    max_=x.max(dim=-1,keepdim=True)[0]\n",
        "    if min_.mean()==0 and max_.mean()==1:\n",
        "        return x.reshape(shape)\n",
        "    \n",
        "    x=(x-min_)/(max_-min_+1e-9)\n",
        "    return x.reshape(shape)\n",
        "\n",
        "def norm_with_clip(x:torch.Tensor,smooth=1e-5):\n",
        "    dim=list(range(1,x.ndim))\n",
        "    mean=x.mean(dim=dim,keepdim=True)\n",
        "    std=x.std(dim=dim,keepdim=True)\n",
        "    x=(x-mean)/(std+smooth)\n",
        "    x[x>5]=(x[x>5]-5)*1e-3 +5\n",
        "    x[x<-3]=(x[x<-3]+3)*1e-3-3\n",
        "    return x\n",
        "\n",
        "def filter_noise(x):\n",
        "    TH=x.reshape(-1)\n",
        "    index = -int(len(TH) * CFG.chopping_percentile)\n",
        "    TH:int = np.partition(TH, index)[index]\n",
        "    x[x>TH]=int(TH)\n",
        "    ########################################################################\n",
        "    TH=x.reshape(-1)\n",
        "    index = -int(len(TH) * CFG.chopping_percentile)\n",
        "    TH:int = np.partition(TH, -index)[-index]\n",
        "    x[x<TH]=int(TH)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(paths,is_label=False):\n",
        "    data_loader=Data_loader(paths,is_label)\n",
        "    data_loader=DataLoader(data_loader, batch_size=16)\n",
        "    data=[]\n",
        "    for x in tqdm(data_loader):\n",
        "        data.append(x)\n",
        "    x=torch.cat(data,dim=0)\n",
        "    del data\n",
        "    if not is_label:\n",
        "      #  ########################################################################\n",
        "      x = filter_noise(x)\n",
        "      x=(min_max_normalization(x.to(torch.float16)[None])[0]*255).to(torch.uint8)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_img_dir=\"data\\\\train\\\\kidney_1_dense\\\\images\"\n",
        "path_lb_dir=\"data\\\\train\\\\kidney_1_dense\\\\labels\"\n",
        "path_img_dir = Path(path_img_dir)\n",
        "path_lb_dir = Path(path_lb_dir)\n",
        "\n",
        "path_img_dir = sorted(list(path_img_dir.rglob(\"*.tif\")))\n",
        "path_lb_dir = sorted(list(path_lb_dir.rglob(\"*.tif\")))\n",
        "\n",
        "images_labels = defaultdict(list)\n",
        "for img in path_img_dir:\n",
        "    images_labels[img.name].append(img)\n",
        "for lb in path_lb_dir:\n",
        "    images_labels[lb.name].append(lb)\n",
        "new_dict = dict(filter(lambda item: len(item[1]) > 1, images_labels.items()))\n",
        "print(f\"\u041e\u0431\u0449\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0441 \u043c\u0430\u0441\u043a\u0430\u043c\u0438 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 : {len(new_dict)}\")\n",
        "img_path = list(map(lambda key : str(new_dict[key][0]) , new_dict))\n",
        "lb_path = list(map(lambda key : str(new_dict[key][1]), new_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x=[]\n",
        "train_y=[]\n",
        "x=load_data(img_path ,is_label=False)\n",
        "\n",
        "y=load_data(lb_path,is_label=True)\n",
        "print(y.shape)\n",
        "train_x.append(x)\n",
        "train_y.append(y)\n",
        "\n",
        "train_x.append(x.permute(1,2,0))\n",
        "train_y.append(y.permute(1,2,0))\n",
        "train_x.append(x.permute(2,0,1))\n",
        "train_y.append(y.permute(2,0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_img_dir=\"data\\\\train\\\\kidney_3_sparse\\\\images\"\n",
        "path_lb_dir=\"data\\\\train\\\\kidney_3_dense\\\\labels\"\n",
        "path_img_dir = Path(path_img_dir)\n",
        "path_lb_dir = Path(path_lb_dir)\n",
        "\n",
        "path_img_dir = sorted(list(path_img_dir.rglob(\"*.tif\")))\n",
        "path_lb_dir = sorted(list(path_lb_dir.rglob(\"*.tif\")))\n",
        "\n",
        "images_labels = defaultdict(list)\n",
        "for img in path_img_dir:\n",
        "    images_labels[img.name].append(img)\n",
        "for lb in path_lb_dir:\n",
        "    images_labels[lb.name].append(lb)\n",
        "new_dict = dict(filter(lambda item: len(item[1]) > 1, images_labels.items()))\n",
        "print(f\"\u041e\u0431\u0449\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0441 \u043c\u0430\u0441\u043a\u0430\u043c\u0438 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 : {len(new_dict)}\")\n",
        "img_path = list(map(lambda key : str(new_dict[key][0]) , new_dict))\n",
        "lb_path = list(map(lambda key : str(new_dict[key][1]), new_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_x=load_data(img_path ,is_label=False)\n",
        "print(val_x.shape)\n",
        "val_y=load_data(lb_path ,is_label=True)\n",
        "print(val_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Kaggld_Dataset(Dataset):\n",
        "    def __init__(self,x:list,y:list,arg=False):\n",
        "        super(Dataset,self).__init__()\n",
        "        self.x=x#list[(C,H,W),...]\n",
        "        self.y=y#list[(C,H,W),...]\n",
        "        self.image_size=CFG.image_size\n",
        "        self.in_chans=CFG.in_chans\n",
        "        self.arg=arg\n",
        "        if arg:\n",
        "            self.transform=CFG.train_aug\n",
        "        else: \n",
        "            self.transform=CFG.valid_aug\n",
        "            \n",
        "    def __len__(self) -> int:\n",
        "        return sum([y.shape[0]-self.in_chans for y in self.y])\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        i=0\n",
        "        for x in self.x:\n",
        "            if index>x.shape[0]-self.in_chans:\n",
        "                index-=x.shape[0]-self.in_chans\n",
        "                i+=1\n",
        "            else:\n",
        "                break\n",
        "        x=self.x[i]\n",
        "        y=self.y[i]\n",
        "        \n",
        "        x_index=np.random.randint(0,x.shape[1]-self.image_size)\n",
        "        y_index=np.random.randint(0,x.shape[2]-self.image_size)\n",
        "\n",
        "        x=x[index:index+self.in_chans,x_index:x_index+self.image_size,y_index:y_index+self.image_size]\n",
        "        y=y[index+self.in_chans//2,x_index:x_index+self.image_size,y_index:y_index+self.image_size]\n",
        "\n",
        "        data = self.transform(image=x.numpy().transpose(1,2,0), mask=y.numpy())\n",
        "        x = data['image']\n",
        "        y = data['mask']>=127\n",
        "        if self.arg:\n",
        "            i=np.random.randint(4)\n",
        "            x=x.rot90(i,dims=(1,2))\n",
        "            y=y.rot90(i,dims=(0,1))\n",
        "            for i in range(3):\n",
        "                if np.random.randint(2):\n",
        "                    x=x.flip(dims=(i,))\n",
        "                    if i>=1:\n",
        "                        y=y.flip(dims=(i-1,))\n",
        "        return x,y#(uint8,uint8)\n",
        "           \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as tc\n",
        "def norm_with_clip(x:tc.Tensor,smooth=1e-5):\n",
        "    dim=list(range(1,x.ndim))\n",
        "    mean=x.mean(dim=dim,keepdim=True)\n",
        "    std=x.std(dim=dim,keepdim=True)\n",
        "    x=(x-mean)/(std+smooth)\n",
        "    x[x>5]=(x[x>5]-5)*1e-3 +5\n",
        "    x[x<-3]=(x[x<-3]+3)*1e-3-3\n",
        "    return x\n",
        "\n",
        "def add_noise(x:tc.Tensor,max_randn_rate=0.1,randn_rate=None,x_already_normed=False):\n",
        "    \"\"\"input.shape=(batch,f1,f2,...) output's var will be normalizate  \"\"\"\n",
        "    ndim=x.ndim-1\n",
        "    if x_already_normed:\n",
        "        x_std=tc.ones([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n",
        "        x_mean=tc.zeros([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n",
        "    else: \n",
        "        dim=list(range(1,x.ndim))\n",
        "        x_std=x.std(dim=dim,keepdim=True)\n",
        "        x_mean=x.mean(dim=dim,keepdim=True)\n",
        "    if randn_rate is None:\n",
        "        randn_rate=max_randn_rate*np.random.rand()*tc.rand(x_mean.shape,device=x.device,dtype=x.dtype)\n",
        "    cache=(x_std**2+(x_std*randn_rate)**2)**0.5\n",
        "    #https://blog.csdn.net/chaosir1991/article/details/106960408\n",
        "    \n",
        "    return (x-x_mean+tc.randn(size=x.shape,device=x.device,dtype=x.dtype)*randn_rate*x_std)/(cache+1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(42)  \n",
        "train_dataset=Kaggld_Dataset(train_x,train_y,arg=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.train_batch_size , shuffle=True )\n",
        "val_dataset=Kaggld_Dataset([val_x],[val_y])\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG.valid_batch_size, shuffle=False, )\n",
        "\n",
        "model=unet.UNet(n_channels=CFG.in_chans, n_classes=CFG.target_size)\n",
        "model = model.cuda()\n",
        "\n",
        "\n",
        "loss_fn=losses.DiceLoss()\n",
        "#DiceLoss()\n",
        "#loss_fn=nn.BCEWithLogitsLoss()\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=CFG.lr)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CFG.lr,\n",
        "                                                steps_per_epoch=len(train_dataset), epochs=CFG.epochs+1,\n",
        "                                                pct_start=0.1,)\n",
        "        \n",
        "\n",
        "train_metrics, val_metrics,train_losses, val_losses = [], [], [],[]\n",
        "best_metric = -np.inf  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(CFG.epochs):\n",
        "    model.train()\n",
        "    \n",
        "    time=tqdm(range(len(train_loader)))\n",
        "    losss=0\n",
        "    scores=0\n",
        "    \n",
        "    for i,(x,y) in enumerate(train_loader):\n",
        "        x=x.cuda().to(tc.float32)\n",
        "        y=y.cuda().to(tc.float32)\n",
        "        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n",
        "        x=add_noise(x,max_randn_rate=0.5,x_already_normed=True)\n",
        "        \n",
        "        pred=model(x)\n",
        "        loss=loss_fn(pred,y)\n",
        "        loss = loss / CFG.n_accumulate\n",
        " \n",
        "        loss.backward()  # loss.backward()  # backward-pass\n",
        "\n",
        "\n",
        "        if (i + 1) % CFG.n_accumulate == 0 or (i + 1 == len(train_loader)):\n",
        "        \n",
        "            optimizer.step()  # update weights\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "        score=dice_coef(pred.detach(),y)\n",
        "        losss=(losss*i+loss.item())/(i+1)\n",
        "        scores=(scores*i+score)/(i+1)\n",
        "        time.set_description(f\"epoch:{epoch},loss:{losss:.4f},score:{scores:.4f},lr{optimizer.param_groups[0]['lr']:.4e}\")\n",
        "        time.update()\n",
        "        del loss,pred\n",
        "    train_losses.append(losss)\n",
        "    train_metrics.append(scores)\n",
        "    \n",
        "    \n",
        "    time.close()\n",
        "    \n",
        "    model.eval()\n",
        "    time=tqdm(range(len(val_dataset)))\n",
        "    val_losss=0\n",
        "    val_scores=0\n",
        "    for i,(x,y) in enumerate(val_loader):\n",
        "        x=x.cuda().to(tc.float32)\n",
        "        y=y.cuda().to(tc.float32)\n",
        "        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n",
        "\n",
        "       \n",
        "        with torch.no_grad():\n",
        "            pred=model(x)\n",
        "            loss=loss_fn(pred,y)\n",
        "        score=dice_coef(pred.detach(),y)\n",
        "        val_losss=(val_losss*i+loss.item())/(i+1)\n",
        "        val_scores=(val_scores*i+score)/(i+1)\n",
        "        time.set_description(f\"val-->loss:{val_losss:.4f},score:{val_scores:.4f}\")\n",
        "        time.update()\n",
        "\n",
        "    time.close()\n",
        "    val_metrics.append(val_scores)\n",
        "    val_losses.append(val_losss)\n",
        "    if val_scores > best_metric:\n",
        "        best_metric = val_scores\n",
        "        save_model(\n",
        "                model=model,\n",
        "                optimizer=optimizer,\n",
        "                model_name=model.__class__.__name__\n",
        "                + \"_best_model_at_\"\n",
        "                + str(epoch + 1),\n",
        "                path='w',\n",
        "                lr_scheduler=scheduler,\n",
        "            )\n",
        "    with open(\"train_results.txt\", \"w\") as file_handler:\n",
        "            file_handler.write(\"train_loss\\n\")\n",
        "            for item in train_losses:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "\n",
        "            file_handler.write(\"\\nval_loss\\n\")\n",
        "            for item in val_losses:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "\n",
        "            file_handler.write(\"\\nval_metrics\\n\")\n",
        "            for item in val_metrics:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "                \n",
        "            file_handler.write(\"\\ntrain_metrics\\n\")\n",
        "            for item in train_metrics:\n",
        "                file_handler.write(\"{}\\t\".format(item))\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "time.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
