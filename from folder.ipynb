{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from segmentation.scr.utils.utils import set_seed\n",
        "from segmentation.scr.utils.metrics import dice_coef\n",
        "from segmentation.models.smp import get_pretrained_model\n",
        "from segmentation.config import CFG\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from segmentation.scr.inference.tta import tta\n",
        "\n",
        "from segmentation.scr.train_model.data_loader import *\n",
        "from segmentation.scr.utils.utils import set_seed, resize_to_size\n",
        "from segmentation.scr.train_model.evaluate_dataset import evaluate_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_my_pc= get_pretrained_model(path_to_model='weight\\mobnet_my_pc.pth', train_parallel=False, CFG=CFG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u041e\u0431\u0449\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0441 \u043c\u0430\u0441\u043a\u0430\u043c\u0438 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 : 501\n"
          ]
        }
      ],
      "source": [
        "path_img_dir=\"data\\\\train\\\\kidney_3_sparse\\\\images\"\n",
        "path_lb_dir=\"data\\\\train\\\\kidney_3_dense\\\\labels\"\n",
        "img_path, lb_path = create_img_lb_paths(path_img_dir = path_img_dir, path_lb_dir = path_lb_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tillings(images, overlap_pct=CFG.tilling_overlap_pct):\n",
        "    if len(images.shape) == 2:\n",
        "        # print('ss')\n",
        "        images = images.unsqueeze(0).repeat(CFG.in_chans, 1, 1)\n",
        "    min_overlap = float(overlap_pct) * 0.01\n",
        "    max_stride = CFG.image_size * (1.0 - min_overlap)\n",
        "\n",
        "    height, width = images.shape[-2], images.shape[-1]\n",
        "    num_patches = np.ceil(np.array([height, width]) / max_stride).astype(np.int32)\n",
        "\n",
        "    starts = [\n",
        "        np.int32(np.linspace(0, height - CFG.input_size, num_patches[0])),\n",
        "        np.int32(np.linspace(0, width - CFG.input_size, num_patches[1])),\n",
        "    ]\n",
        "    stops = [starts[0] + CFG.input_size, starts[1] + CFG.input_size]\n",
        "\n",
        "    indexs = []\n",
        "    tills = []\n",
        "    for y1, y2 in zip(starts[0], stops[0]):\n",
        "        for x1, x2 in zip(starts[1], stops[1]):\n",
        "            tills.append(\n",
        "                    images[..., y1:y2, x1:x2]\n",
        "            )\n",
        "            indexs.append((y1, y2, x1, x2))\n",
        "\n",
        "    return tills, indexs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Data_loader_inference(Dataset):\n",
        "    def __init__(self,img_path,lb_path):\n",
        "        self.img_paths=img_path\n",
        "        self.lb_path = lb_path\n",
        "        h_m, w_m = 0,0\n",
        "        for path in self.img_paths:\n",
        "            img=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "            h, w  = img.shape\n",
        "            h_m = max(h, h_m)\n",
        "            w_m = max(w, w_m)\n",
        "        self.img_size = (h_m, w_m)\n",
        "        \n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return sum([len(self.lb_path)- CFG.in_chans for y in self.lb_path])\n",
        "        #return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        paths_img  = self.img_paths[index : index +  CFG.in_chans]\n",
        "        path_lb  = self.lb_path[index + CFG.in_chans // 2]\n",
        "        \n",
        "        images = []\n",
        "\n",
        "        for path_img in paths_img:\n",
        "            img=cv2.imread(path_img,cv2.IMREAD_GRAYSCALE)\n",
        "            h, w  = img.shape\n",
        "            img=torch.from_numpy(img).to(torch.uint8)\n",
        "            if h < self.img_size[0] or w < self.img_size[1]:\n",
        "                img = resize_to_size(img=img, image_size=self.img_size)\n",
        "            images.append(img)\n",
        "            \n",
        "        label = cv2.imread(path_lb,cv2.IMREAD_GRAYSCALE)\n",
        "        label=torch.from_numpy(label!=0).to(torch.uint8)*255\n",
        "        if h < self.img_size[0] or w < self.img_size[1]:  \n",
        "            label = resize_to_size(img=label, image_size=self.img_size)\n",
        "        images= torch.stack(images)\n",
        "      \n",
        "\n",
        "        return images, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader_inf = Data_loader_inference(img_path, lb_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loader = DataLoader(loader_inf, batch_size=2, shuffle=False, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "images, masks = batch\n",
        "images = (min_max_normalization(images.to(torch.float16)[None])[0]*255).to(torch.uint8)\n",
        "tills, indexs = create_tillings(images=images, overlap_pct=10)\n",
        "till_shape = len(tills)\n",
        "tills = torch.cat(tills)\n",
        "tills =tills.to(torch.float32)\n",
        "tills = norm_with_clip(tills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_my_pc.eval()\n",
        "with torch.no_grad():\n",
        "    ans= tta(model=model_my_pc, x=tills.cuda().to(torch.float32))\n",
        "ans = ans.cpu()\n",
        "#ans = ans.sigmoid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "ans = ans.reshape(till_shape, 2, CFG.input_size, CFG.input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask_pred = torch.zeros_like(masks,dtype=torch.float32)\n",
        "mask_count = torch.zeros_like(masks,dtype=torch.float32)\n",
        "for i,(y1,y2,x1,x2) in enumerate(indexs):\n",
        "    mask_pred[...,y1:y2, x1:x2] += ans[i]\n",
        "    mask_count[...,y1:y2, x1:x2] += 1\n",
        " \n",
        "mask_pred /= mask_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.8928)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dice_coef(mask_pred[1], masks > 0, from_logits= False, thr=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im1 = loader_inf[0][0]\n",
        "im1 = (min_max_normalization(im1.to(torch.float16)[None])[0]*255).to(torch.uint8)\n",
        "tills, indexs = create_tillings(images=im1.repeat(CFG.in_chans,1,1))\n",
        "img_shape = tills[0].shape\n",
        "tills = torch.cat(tills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "11 % 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "15 + (3 - 15% 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "11 // 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lb_path[500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_x=load_data(img_path ,is_label=False)\n",
        "print(val_x.shape)\n",
        "val_y=load_data(lb_path ,is_label=True)\n",
        "print(val_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "val_dataset_3=Kaggld_Dataset([val_x],[val_y], arg=False, CFG=CFG_3)\n",
        "val_loader_3 = DataLoader(val_dataset_3, batch_size=CFG.valid_batch_size, shuffle=False, )\n",
        "\n",
        "val_dataset_5=Kaggld_Dataset([val_x],[val_y], arg=False, CFG=CFG)\n",
        "val_loader_5 = DataLoader(val_dataset_5, batch_size=CFG.valid_batch_size, shuffle=False, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tta(x, model):\n",
        "    model.eval()\n",
        "    x_n=[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(4)]\n",
        "\n",
        "    shape = x.shape\n",
        "    for i in range(4):\n",
        "        if i ==0:\n",
        "            x_n[i] = x_n[i]\n",
        "        else:\n",
        "            x_n[i]=torch.flip(x_n[i] , dims = (-2,))\n",
        "    with torch.no_grad():\n",
        "        pred = [model(b) for b in x_n]\n",
        "    \n",
        "        torch.flip(pred[i] , dims = (-2,))\n",
        "    pred=torch.cat(pred,dim=0)\n",
        "    pred = pred.sigmoid()\n",
        "    pred =pred.reshape(4,shape[0],*shape[2:])\n",
        "    \n",
        "    \n",
        "    for i in range(4):\n",
        "        if i ==0:\n",
        "            pred[i] = pred[i]\n",
        "        else:\n",
        "            pred[i]=torch.flip(pred[i] , dims = (-2,))\n",
        "    pred=[torch.rot90(pred[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
        "    pred=torch.stack(pred,dim=0).mean(0)\n",
        "            \n",
        "\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tilling( images, overlap_pct = 50):\n",
        "    if len(images.shape) == 2:\n",
        "        #print('ss')\n",
        "        images = images.unsqueeze(0).repeat(CFG.in_chans,1,1)\n",
        "    min_overlap = float(overlap_pct) * 0.01\n",
        "    max_stride = CFG.image_size * (1.0 - min_overlap)\n",
        "\n",
        "    #print(images.shape)\n",
        "    height, width = images.shape[1], images.shape[2]\n",
        "    num_patches = np.ceil(np.array([height, width]) / max_stride).astype(\n",
        "                np.int32\n",
        "            )\n",
        "\n",
        "    starts = [\n",
        "                np.int32(np.linspace(0, height - CFG.input_size, num_patches[0])),\n",
        "                np.int32(np.linspace(0, width - CFG.input_size, num_patches[1])),\n",
        "            ]\n",
        "    stops = [starts[0] + CFG.input_size, starts[1] + CFG.input_size]\n",
        "        \n",
        "    indexs=[]\n",
        "    tills=[]\n",
        "    for y1, y2 in zip(starts[0], stops[0]):\n",
        "        for x1, x2 in zip(starts[1], stops[1]):\n",
        "            tills.append(images[...,y1:y2, x1:x2])\n",
        "            indexs.append((y1,y2,x1,x2))\n",
        "                \n",
        "  \n",
        "    return tills, indexs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_dataset_2(model, val_loader):\n",
        "    model.eval()\n",
        "    timer = tqdm(range(len(val_loader)))\n",
        "    val_scores = 0\n",
        "    scores = []\n",
        "    for i, (x, y) in enumerate(val_loader):\n",
        "        x = x.cuda().to(torch.float32)\n",
        "        y = y.cuda().to(torch.float32)\n",
        "        x = norm_with_clip(x.reshape(-1, *x.shape[2:])).reshape(x.shape)\n",
        "        with torch.no_grad():\n",
        "            pred = tta(model=model, x=x)\n",
        "\n",
        "        score = dice_coef(pred.detach(), y, from_logits=False)\n",
        "        scores.append(score.detach().cpu().item())\n",
        "        val_scores = (val_scores * i + score) / (i + 1)\n",
        "        timer.set_description(f\"eval--> score:{val_scores:.4f}\")\n",
        "        timer.update()\n",
        "    timer.close()\n",
        "    return scores, val_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_2, val_scores_2 = evaluate_dataset_2(model=model_my_pc, val_loader=val_loader_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(val_scores_2, val_scores_my)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_my, val_scores_my= evaluate_dataset(model=model_my_pc ,val_loader=val_loader_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "scipy.stats.ttest_rel(a=scores_my, b=scores_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
